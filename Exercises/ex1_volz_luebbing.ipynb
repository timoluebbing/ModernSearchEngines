{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 4271 - Exercise 1 - Web Crawling\n",
    "\n",
    "Issued: April 16, 2024\n",
    "\n",
    "Due: April 22, 2024\n",
    "\n",
    "Please submit this filled sheet via Ilias by the due date.\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Duplicate Detection\n",
    "When crawling large numbers of Web pages we are likely to encounter a considerable number of duplicate documents. To not flood our index with replicas of the same documents, we need a duplicate detection scheme.\n",
    "\n",
    "a) Using python's built-in hash() function, process the following documents in order of appearance and flag up any exact duplicates.\n",
    "\n",
    "- **D1** \"This is just some document\"\n",
    "- **D2** \"This is another piece of text\"\n",
    "- **D3** \"This is another piece of text\"\n",
    "- **D4** \"This is just some documents\"\n",
    "- **D5** \"Totally different stuff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check a single document against an existing collection of previsouly seen documents for exact duplicates.\n",
    "def check_exct(doc, docs):\n",
    "    docs = [d[1] for d in docs]\n",
    "    return hash(doc[1]) in (hash(d) for d in docs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Going beyond exact duplicates, we want to also identify any near-duplicates that are very similar but not identical to previously seen content. Implement the SimHash method discussed in class and again process the five documents, this time flagging up exact and near duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "# example set of stopwords \n",
    "# (instead of using nltk stopwords list for simplicity)\n",
    "stopwords = set(['.', ',', 'i', 'the', 'of', '...']) \n",
    "\n",
    "def create_simhash(doc) -> list:\n",
    "    # determine document word frequency\n",
    "    word_freq = {} #dictionary of word frequency\n",
    "    for word in doc.split():\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "    # create  8 bit hash value for each word, convert 0 to -1 and multiply by frequency \n",
    "    weighted_hash_values = []\n",
    "    for word in word_freq:\n",
    "        hash_object = hashlib.md5(word.encode('utf-8')) \n",
    "        hash_value = int(hash_object.hexdigest(), 16) % 256\n",
    "        hash_value = [int(b) for b in bin(hash_value)[2:].zfill(8)]\n",
    "        # turn each 0 into -1 \n",
    "        hash_value = [x if x > 0 else -1 for x in hash_value]\n",
    "        # multiply hash value by word frequency\n",
    "        weighted_hash_value = [hv * word_freq[word] for hv in hash_value] \n",
    "        weighted_hash_values.append(weighted_hash_value)\n",
    "    \n",
    "    # print(weighted_hash_values)\n",
    "    summed_hash_values = [sum(x) for x in zip(*weighted_hash_values)]\n",
    "    # convert > 0 to 1 and 0 otherwise\n",
    "    summed_hash_values = [1 if x > 0 else 0 for x in summed_hash_values]\n",
    "    # sum each hash_value \n",
    "    return summed_hash_values \n",
    "\n",
    "#Check a single document against an existing collection of previsouly seen documents for near duplicates\n",
    "def check_simhash(doc, docs):\n",
    "    docs = [d[1] for d in docs]\n",
    "    doc = doc[1]\n",
    "    print(doc, docs)\n",
    "    simhash_doc = create_simhash(doc)\n",
    "    for d in docs:\n",
    "        simhash_d = create_simhash(d)\n",
    "        hamming_distance = sum([1 for i in range(len(simhash_doc)) if simhash_doc[i] != simhash_d[i]])\n",
    "        if hamming_distance < 3:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is just some document []\n",
      "This is another piece of text ['This is just some document']\n",
      "This is another piece of text ['This is just some document', 'This is another piece of text']\n",
      "DUPLICATE: D3\n",
      "This is just some documents ['This is just some document', 'This is another piece of text']\n",
      "DUPLICATE: D4\n",
      "Totally different stuff ['This is just some document', 'This is another piece of text']\n",
      "[['D1', 'This is just some document'], ['D2', 'This is another piece of text'], ['D5', 'Totally different stuff']]\n"
     ]
    }
   ],
   "source": [
    "crawl = [['D1', 'This is just some document'], ['D2', 'This is another piece of text'], ['D3', 'This is another piece of text'], ['D4', 'This is just some documents'], ['D5', 'Totally different stuff']]\n",
    "\n",
    "#Process raw crawled website content\n",
    "def process(crawl):\n",
    "    docs = []\n",
    "    for doc in crawl:\n",
    "        if check_simhash(doc, docs): #Can be exchanged for check_simhash()\n",
    "            print('DUPLICATE: '+doc[0])\n",
    "        else:\n",
    "            docs.append(doc)\n",
    "    print(docs)\n",
    "process(crawl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Focused Search Engines\n",
    "Suppose you were to build a COVID-19 Web search engine for which you want to collect and eventually serve only COVID-19 information. The general web crawling process follows this scheme:\n",
    "\n",
    "1. Create a seed set of known URLs (a.k.a the frontier)\n",
    "2. Pull a URL from the frontier and visit it\n",
    "3. Save the page content for our search engine (indexing)\n",
    "4. Once on the page, note down all URLs linked there\n",
    "5. Put all encountered URLs in the queue\n",
    "6. Repeat from Step 2 until the queue is empty\n",
    "\n",
    "In this particular setting, how should the generic step-by-step crawling process be modified/extended? Discuss all relevant considerations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General considerations:\n",
    "1. Do not revisit the same pages\n",
    "2. Do not crawl pages with almost identical content\n",
    "3. Be polite: do not revisit same page too often within a single timeframe\n",
    "\n",
    "#### COVID-19 specific considerations:\n",
    "1. Content saving: When saving the page content, a filter should be applied to ensure the content is relevant to COVID-19. This could involve checking for COVID-19 related keywords, and similarly...\n",
    "2. Link extraction: Not all URLs from a website should be added to the queue. Only URLs that are likely to lead to COVID-19 related information should be added.\n",
    "3. Repetition/Freshness: To ensure the freshness of the information, the crawler should periodically revisit the URLs in the seed set and any other URLs that have been identified as important sources of COVID-19 information. E.g. flag URLs as important during the crawing process. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
